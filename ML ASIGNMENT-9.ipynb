{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8a54c6",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n",
    "\n",
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?\n",
    "\n",
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.\n",
    "\n",
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n",
    "\n",
    "9. Make a few quick notes on:\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n",
    "\n",
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f2fea",
   "metadata": {},
   "source": [
    "1.Feature Engineering is a process where raw data is converted into useful features so that the accuracy of the model will increase.Following are the methods for feature engineering:\n",
    "i>feature creation (create new feature using the existing feature)\n",
    "ii>feature extraction\n",
    "iii>feature selection\n",
    "iv>feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a40867",
   "metadata": {},
   "source": [
    "2.FEATURE SELECTION IS TAKING THAT FEATURE ONLY WHICH ARE MOST IMPORTANT AND CAN INCREASE THE ACCURACY OF THE MODEL.\n",
    "  THERE IS DIFFERENT DIFFERENT TECHNIQUES FOR FEATURE SELECTION ARE:WRAPPER METHOD, FILTER METHOD AND EMBEDDED METHOD.\n",
    "  Filter methods select features based on their information gain, chi-square test, Fisher’s score, and other criteria 2. Wrapper   methods select features greedily by adding or removing them based on the performance of the learning algorithm 2. Embedded       methods  select features as part of the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d954914",
   "metadata": {},
   "source": [
    "3.Function selection is a process of choosing a subset of features from the original features that are most relevant and useful for a machine learning task. There are two main approaches for function selection: filter and wrapper methods\n",
    "Filter methods select features based on some statistical criteria, such as correlation, information gain, chi-square, etc. They are independent of the machine learning algorithm and are usually fast and scalable. However, they do not consider the interaction between features and the classifier, and may miss some important features that are only relevant in combination with others.\n",
    "\n",
    "Wrapper methods select features based on the performance of a specific machine learning algorithm. They use a search strategy, such as greedy search, genetic algorithms, or simulated annealing, to find the optimal subset of features that maximizes the classifier accuracy. They are able to capture the interaction between features and the classifier, and often produce better results than filter methods. However, they are computationally expensive, prone to overfitting, and may not generalize well to other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb3eb8",
   "metadata": {},
   "source": [
    "4.Feature extraction is a process of transforming raw data into numerical features that are compatible with machine learning algorithms. The key underlying principle of feature extraction is to reduce the dimensionality and complexity of the data, while preserving the relevant information for the task at hand. For example, in image processing, feature extraction techniques can be used to extract the shape, color, texture, or edges of an object from an image, instead of using the raw pixel values.\n",
    "Widely usef function extraction algorithms are:\n",
    "i.Bag of words\n",
    "ii.Principal component analysis(PCA)\n",
    "iii.Local binary pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc673e59",
   "metadata": {},
   "source": [
    "5.The feature engineering process for text categorization typically involves the following steps:\n",
    "\n",
    "Text preprocessing: This step involves cleaning and standardizing the text data by removing punctuation, stopwords, numbers, HTML tags, etc. It also involves applying techniques such as stemming, lemmatization, tokenization, etc. to reduce the vocabulary size and extract meaningful units of text.\n",
    "Text representation: This step involves converting the preprocessed text into numerical vectors that can capture the semantic and syntactic information of the text. There are various methods for text representation, such as bag-of-words, term frequency-inverse document frequency (TF-IDF), word embeddings, etc.\n",
    "Feature selection: This step involves selecting the most relevant and informative features from the text representation that can help improve the text categorization performance. Feature selection can reduce the dimensionality, noise, and redundancy of the text data, and enhance the generalization ability of the machine learning models. There are various methods for feature selection, such as chi-square, information gain, mutual information, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686d12e",
   "metadata": {},
   "source": [
    "6.Cosine similarity is a good metric for text categorization because it measures the angle between two vectors, which reflects how similar the documents are in terms of their word distributions. Cosine similarity is independent of the document length and only depends on the orientation of the vectors.\n",
    "To find the cosine similarity between two rows of a document-term matrix, we can use the formula:\n",
    "cosine similarity=∥a∥∥b∥a⋅b​\n",
    "where a and b are the row vectors, ⋅ is the dot product, and ∥⋅∥ is the norm.\n",
    "Using this formula, we can calculate the cosine similarity between the two rows given as:\n",
    "cosine similarity=∥(2,3,2,0,2,3,3,0,1)∥∥(2,1,0,0,3,2,1,3,1)∥(2,3,2,0,2,3,3,0,1)⋅(2,1,0,0,3,2,1,3,1)​\n",
    "cosine similarity=62​32​26​\n",
    "cosine similarity≈0.71\n",
    "This means that the two rows have a high degree of similarity, as the cosine of their angle is close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6422b9",
   "metadata": {},
   "source": [
    "7.The Hamming distance is a metric that measures the number of positions at which the bits of two binary strings are different. The formula for calculating the Hamming distance between two binary strings a and b of equal length n is:\n",
    "d(a,b)=i=1∑n​(ai​=bi​)\n",
    "where ai​ and bi​ are the bits at the i-th position of a and b, respectively12.\n",
    "To calculate the Hamming distance between 10001011 and 11001111, we can use the XOR operation, which returns 1 when the bits are different and 0 when they are the same. Then, we count the number of 1s in the result13.\n",
    "10001011⊕11001111d(10001011,11001111)​=01000100=2​\n",
    "The Hamming distance between 10001011 and 11001111 is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5e720",
   "metadata": {},
   "source": [
    "ii.The Jaccard index and the similarity matching coefficient are two measures of similarity between two sets of binary data. They are calculated as follows:\n",
    "\n",
    "Jaccard index: J(A,B)=∣A∪B∣∣A∩B∣​\n",
    "\n",
    "Similarity matching coefficient: S(A,B)=∣A∪B∣∣A∩B∣+∣Ac∩Bc∣​\n",
    "\n",
    "\n",
    "where A and B are the two sets, ∣X∣ is the cardinality of set X, and Xc is the complement of set X.\n",
    "For the given values, we can represent them as two sets:\n",
    "\n",
    "A={1,2,5,7,8}\n",
    "B={1,2,6,7,8}\n",
    "\n",
    "Then, we can compute the Jaccard index and the similarity matching coefficient as follows:\n",
    "\n",
    "Jaccard index: J(A,B)=∣{1,2,5,6,7,8}∣∣{1,2,7,8}∣​=64​=0.6667\n",
    "\n",
    "Similarity matching coefficient: S(A,B)=∣{1,2,5,6,7,8}∣∣{1,2,7,8}∣+∣{3,4}∣​=64+2​=1\n",
    "\n",
    "\n",
    "The Jaccard index and the similarity matching coefficient have different properties and interpretations. The Jaccard index only considers the positive matches between the two sets, while the similarity matching coefficient also considers the negative matches. The Jaccard index ranges from 0 to 1, where 0 means no similarity and 1 means perfect similarity. The similarity matching coefficient ranges from 0.5 to 1, where 0.5 means no similarity and 1 means perfect similarity. The similarity matching coefficient is always greater than or equal to the Jaccard index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832ed2c",
   "metadata": {},
   "source": [
    "8.A high-dimensional data set is a data set in which the number of features or attributes is larger than the number of observations or samples1. For example, a data set that has 6 features and only 3 observations would be considered high-dimensional.\n",
    "\n",
    "Some real-life examples of high-dimensional data sets are:\n",
    "\n",
    "Healthcare data, where the features can include various measurements, tests, and diagnoses for each patient1.\n",
    "Financial data, where the features can include various indicators, ratios, and prices for each stock.\n",
    "Genomics data, where the features can include thousands of genes for each individual.\n",
    "One of the main difficulties in using machine learning techniques on high-dimensional data sets is the curse of dimensionality. This means that as the number of features increases, the data becomes sparser and more complex, making it harder to find patterns, generalize from the data, and avoid overfittin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089334f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
